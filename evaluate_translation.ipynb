{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, I will evaluate the performance of the trained NMT, including:\n",
    "- BLEU: widely-used metric that measures n-gram overlap between predicted sentence and true sentence.\n",
    "- TER (Translation Error Rate): measuring number of editing required to transform a predicted sentence to true sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:   print(e)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "from utils.read_file_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.evaluation_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FILE_TEST_EN = r\"data/processed_data/en_sent_test.txt\"\n",
    "PATH_FILE_TEST_VI = r\"data/processed_data/vi_sent_test.txt\"\n",
    "\n",
    "PATH_MODEL_TRANSLATOR = \"translator\"\n",
    "\n",
    "translator = tf.saved_model.load(PATH_MODEL_TRANSLATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence: 10000\n"
     ]
    }
   ],
   "source": [
    "N_samples = 10_000\n",
    "\n",
    "list_en_sentence = read_text_file(PATH_FILE_TEST_EN)\n",
    "list_vi_sentence = read_text_file(PATH_FILE_TEST_VI)\n",
    "\n",
    "if N_samples != None:\n",
    "    list_en_sentence = list_en_sentence[:N_samples]\n",
    "    list_vi_sentence = list_vi_sentence[:N_samples]\n",
    "\n",
    "assert len(list_en_sentence) == len(list_vi_sentence)\n",
    "print(f\"Number of sentence: {len(list_en_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: - hush up , lottie .\n",
      "Vietname: - yên nào , lottie .\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(list_en_sentence))\n",
    "\n",
    "en_sentence = list_en_sentence[idx]\n",
    "vi_sentence = list_vi_sentence[idx]\n",
    "\n",
    "print(f\"English: {en_sentence}\")\n",
    "print(f\"Vietname: {vi_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Test BLEU on single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: and i understand that if she slipped up that she would have a completely reasonable explanation for it .\n",
      "Vietname: và nhỡ có mắc sai lầm thì cô ta sẽ có một lời giải thích hợp lý .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Translated text: và tôi hiểu rằng nếu cô ấy bị trượt chân , cô ấy sẽ có một lời giải thích hoàn toàn hợp lý cho nó .\n",
      "BLEU score: 0.21675453206953177\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(list_en_sentence))\n",
    "\n",
    "en_sentence = list_en_sentence[idx]\n",
    "en_sentence = en_sentence.lower()\n",
    "vi_sentence = list_vi_sentence[idx]\n",
    "vi_sentence = vi_sentence.lower()\n",
    "\n",
    "print(f\"English: {en_sentence}\")\n",
    "print(f\"Vietname: {vi_sentence}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "translated_text, translated_tokens, attention_weights, list_predicted_prob = translator(tf.constant(en_sentence))\n",
    "translated_text = translated_text.numpy().decode('utf-8')\n",
    "print(f\"Translated text: {translated_text}\")\n",
    "\n",
    "score = calculate_bleu_score([vi_sentence], translated_text)\n",
    "print(\"BLEU score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. BLEU on all Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx = 0\n",
      "idx = 1000\n",
      "idx = 2000\n",
      "idx = 3000\n",
      "idx = 4000\n",
      "idx = 5000\n",
      "idx = 6000\n",
      "idx = 7000\n",
      "idx = 8000\n",
      "idx = 9000\n",
      "Average BLEU score: 0.27079152411186236\n"
     ]
    }
   ],
   "source": [
    "list_bleu_scores = []\n",
    "\n",
    "for idx, (en_sentence, vi_sentence) in enumerate(zip(list_en_sentence, list_vi_sentence)):\n",
    "    if idx % 1_000 == 0:\n",
    "        print(f\"idx = {idx}\")\n",
    "\n",
    "    translated_text, translated_tokens, attention_weights, list_predicted_prob = translator(tf.constant(en_sentence))\n",
    "    translated_text = translated_text.numpy().decode('utf-8')\n",
    "    score = calculate_bleu_score([vi_sentence], translated_text)\n",
    "    list_bleu_scores.append(score)\n",
    "\n",
    "print(f\"Average BLEU score: {sum(list_bleu_scores)/len(list_bleu_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Translation Error Rate \n",
    "\n",
    "Type of edits in TER:\n",
    "- Insertions.\n",
    "- Deletion.\n",
    "- Substitution. \n",
    "\n",
    "TER calculates the number of edits needed to turn the translated output into a true translation, normalized by the length of the reference. It’s often expressed as a percentage, with lower TER values indicating better translation quality (i.e., less editing required):\n",
    "- 0 TER is the best. \n",
    "- 1 TER is the worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Test TER on single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: i design silicon lithography for personal gain .\n",
      "Vietname: tôi thiết kế nên phương pháp khắc quang phổ lên nhựa silicon .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Translated text: tôi thiết kế in in silicon để có được lợi ích cá nhân .\n",
      "TER Score: 0.7692\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(list_en_sentence))\n",
    "\n",
    "en_sentence = list_en_sentence[idx]\n",
    "en_sentence = en_sentence.lower()\n",
    "vi_sentence = list_vi_sentence[idx]\n",
    "vi_sentence = vi_sentence.lower()\n",
    "\n",
    "print(f\"English: {en_sentence}\")\n",
    "print(f\"Vietname: {vi_sentence}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "translated_text, translated_tokens, attention_weights, list_predicted_prob = translator(tf.constant(en_sentence))\n",
    "translated_text = translated_text.numpy().decode('utf-8')\n",
    "print(f\"Translated text: {translated_text}\")\n",
    "\n",
    "ter_score = calculate_ter(vi_sentence, translated_text)\n",
    "print(f\"TER Score: {ter_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. TER on all Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx = 0\n",
      "idx = 1000\n",
      "idx = 2000\n",
      "idx = 3000\n",
      "idx = 4000\n",
      "idx = 5000\n",
      "idx = 6000\n",
      "idx = 7000\n",
      "idx = 8000\n",
      "idx = 9000\n",
      "Average BLEU score: 0.572375288929681\n"
     ]
    }
   ],
   "source": [
    "list_ter_scores = []\n",
    "\n",
    "for idx, (en_sentence, vi_sentence) in enumerate(zip(list_en_sentence, list_vi_sentence)):\n",
    "    if idx % 1_000 == 0:\n",
    "        print(f\"idx = {idx}\")\n",
    "\n",
    "    translated_text, translated_tokens, attention_weights, list_predicted_prob = translator(tf.constant(en_sentence))\n",
    "    translated_text = translated_text.numpy().decode('utf-8')\n",
    "    ter_score = calculate_ter(vi_sentence, translated_text)\n",
    "    list_ter_scores.append(ter_score)\n",
    "\n",
    "print(f\"Average BLEU score: {sum(list_ter_scores)/len(list_ter_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
