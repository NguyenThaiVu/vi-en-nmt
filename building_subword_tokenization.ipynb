{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, we will generate a subword vocabulary from a dataset in both English and Vietnamese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 16:57:53.415819: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-27 16:57:53.415845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-27 16:57:53.435842: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:   print(e)\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "from read_file_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_EN_FILE = r\"data/processed_data/en_sent.txt\"\n",
    "PATH_VI_FILE = r\"data/processed_data/vi_sent.txt\"\n",
    "\n",
    "# PATH_FOLDER_PROCESS = r\"data/processed_data\"\n",
    "PATH_FOLDER_VOCAB = r\"data/vocab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pair sentence: 3265005\n"
     ]
    }
   ],
   "source": [
    "list_en_sentence = read_text_file(PATH_EN_FILE)\n",
    "list_vi_sentence = read_text_file(PATH_VI_FILE)\n",
    "\n",
    "assert len(list_en_sentence) == len(list_vi_sentence)\n",
    "print(f\"Number of pair sentence: {len(list_en_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = tf.data.Dataset.from_tensor_slices(list_en_sentence)\n",
    "train_vi = tf.data.Dataset.from_tensor_slices(list_vi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:    it begins with a countdown .\n",
      "Vietnamese:    c√¢u chuy·ªán b·∫Øt ƒë·∫ßu v·ªõi bu·ªïi l·ªÖ ƒë·∫øm ng∆∞·ª£c .\n"
     ]
    }
   ],
   "source": [
    "for en, vi in zip(train_en, train_vi):\n",
    "    print(\"English:   \", en.numpy().decode('utf-8'))\n",
    "    print(\"Vietnamese:   \", vi.numpy().decode('utf-8'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate vocabulary\n",
    "\n",
    "- This section generates a vocabulary from a dataset. \n",
    "- The vocabulary here mean the list of subword token, which use the `BertTokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. English Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer_params=dict(lower_case=True)\n",
    "bert_tokenizer_params=dict()\n",
    "RESERVED_TOKENS=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "VOCAB_SIE = 100_000\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    vocab_size = VOCAB_SIE,\n",
    "    reserved_tokens=RESERVED_TOKENS,  # Reserved tokens that must be included in the vocabulary\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 56s, sys: 2.59 s, total: 6min 59s\n",
      "Wall time: 5min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab_from_dataset.bert_vocab_from_dataset(\n",
    "    train_en.batch(1000).prefetch(tf.data.AUTOTUNE),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '#', '$', '%', '&']\n",
      "['from', 'but', 'me', 'an', 're', 'she', 'so', 'my', 'will', 'all']\n",
      "['##Ìôî', '##Ô¨Å', '##Ô¨Ç', '##Ô∏è', '##Ôºä', '##Ôºã', '##Ôºå', '##Ôºç', '##ÔøΩ', '##íÄ≠']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:10])\n",
    "print(en_vocab[1000:1010])\n",
    "print(en_vocab[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can save the `en_vocab` for later application usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)\n",
    "\n",
    "write_vocab_file(os.path.join(PATH_FOLDER_VOCAB, 'en_vocab.txt'), en_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Vietnamese vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 48s, sys: 2.16 s, total: 6min 50s\n",
      "Wall time: 5min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vi_vocab = bert_vocab_from_dataset.bert_vocab_from_dataset(\n",
    "    train_vi.batch(1000).prefetch(tf.data.AUTOTUNE),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '#', '$', '%', '&']\n",
      "['s·∫Ω', 'ch√∫ng', 'nh·ªØng', 'ta', 'ƒë·ªÉ', 'n√†y', 'khi', 'l√†m', 'v√†o', 'ra']\n",
      "['##Ìï¥', '##ÌòÑ', '##Ìò∏', '##Ìôî', '##Ô∏è', '##Ôºã', '##Ôºå', '##Ôøº', '##ÔøΩ', '##íÄ≠']\n"
     ]
    }
   ],
   "source": [
    "print(vi_vocab[:10])\n",
    "print(vi_vocab[1000:1010])\n",
    "print(vi_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file(os.path.join(PATH_FOLDER_VOCAB, 'vi_vocab.txt'), vi_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build tokenizer\n",
    "\n",
    "- After extracting the vocabulary, we can build the Tokenizer from it. \n",
    "- We can use the function `tf_text.BertTokenizer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = tf_text.BertTokenizer(os.path.join(PATH_FOLDER_VOCAB, 'en_vocab.txt'))\n",
    "vi_tokenizer = tf_text.BertTokenizer(os.path.join(PATH_FOLDER_VOCAB, 'vi_vocab.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can test the `tokenizer`, which take input as string and return the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese token index: <tf.RaggedTensor [[1040, 1445, 1645, 1434, 1420, 2881, 1082]]>\n"
     ]
    }
   ],
   "source": [
    "vi_test = 'th√†nh ph·ªë h·ªì ch√≠ minh ng·∫≠p n∆∞·ªõc'\n",
    "vi_token_idx = vi_tokenizer.tokenize(vi_test)\n",
    "vi_token_idx = vi_token_idx.merge_dims(-2, -1)\n",
    "print(f\"Vietnamese token index: {vi_token_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can look back from the token index to the original text, by using `tf.gather`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output ot detokenizer: <tf.RaggedTensor [[b'th\\xc3\\xa0nh', b'ph\\xe1\\xbb\\x91', b'h\\xe1\\xbb\\x93', b'ch\\xc3\\xad',\n",
      "  b'minh', b'ng\\xe1\\xba\\xadp', b'n\\xc6\\xb0\\xe1\\xbb\\x9bc']]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'th√†nh ph·ªë h·ªì ch√≠ minh ng·∫≠p n∆∞·ªõc'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "# txt_tokens = tf.gather(vi_vocab, vi_token_idx)\n",
    "txt_tokens = vi_tokenizer.detokenize(vi_token_idx)\n",
    "print(f\"Output ot detokenizer: {txt_tokens}\")\n",
    "\n",
    "# Join with spaces.\n",
    "original_str = tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1).numpy()[0].decode('utf-8')\n",
    "original_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Custom and Export\n",
    "\n",
    "We will export the tokenizer using `tf.saved_model` so they can be imported by other application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(RESERVED_TOKENS) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(RESERVED_TOKENS) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    \"\"\"\n",
    "    This function take batch of token index and add index of START and END token\n",
    "    \"\"\"\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    \"\"\"\n",
    "    This function take list of token and return the complete sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we will define the custom `tf.Module`, which can be use in later part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    self.tokenizer = tf_text.BertTokenizer(vocab_path)\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    ## Create the signatures for export:   \n",
    "\n",
    "    # Include a tokenize signature for a batch of strings. \n",
    "    self.tokenize.get_concrete_function(tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "    # Include `detokenize` and `lookup` signatures for:\n",
    "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "    #   * `RaggedTensors` with shape [batch, tokens]\n",
    "    self.detokenize.get_concrete_function(tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    # These `get_*` methods take no arguments\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "    enc = add_start_end(enc)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we build the `CustomTokenizer` for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.en = CustomTokenizer(RESERVED_TOKENS, os.path.join(PATH_FOLDER_VOCAB, 'en_vocab.txt'))\n",
    "tokenizers.vi = CustomTokenizer(RESERVED_TOKENS, os.path.join(PATH_FOLDER_VOCAB, 'vi_vocab.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/tokeninzer_en_vi_converter/assets\n"
     ]
    }
   ],
   "source": [
    "model_name = os.path.join('data', 'tokeninzer_en_vi_converter')\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test tokenizer\n",
    "\n",
    "To make sure our tokenizer work correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,   50, 1155,  972,    3]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_test = 'i love you'\n",
    "\n",
    "tokens = tokenizers.en.tokenize([en_test])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can `lookup` each token index and return corresponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'i', b'love', b'you', b'[END]']]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = tokenizers.en.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the `detokenize` function to convert back to original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you\n"
     ]
    }
   ],
   "source": [
    "round_trip = tokenizers.en.detokenize(tokens)\n",
    "\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 1853, 1563,  988, 2924,  989, 1160,    4,    3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_test = 'h√† n·ªôi kh√¥ng v·ªôi ƒë∆∞·ª£c ƒë√¢u!'\n",
    "\n",
    "tokens = tokenizers.vi.tokenize([vi_test])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can `lookup` each token index and return corresponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'h\\xc3\\xa0', b'n\\xe1\\xbb\\x99i', b'kh\\xc3\\xb4ng',\n",
       "  b'v\\xe1\\xbb\\x99i', b'\\xc4\\x91\\xc6\\xb0\\xe1\\xbb\\xa3c',\n",
       "  b'\\xc4\\x91\\xc3\\xa2u', b'!', b'[END]']]>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = tokenizers.vi.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the `detokenize` function to convert back to original sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h√† n·ªôi kh√¥ng v·ªôi ƒë∆∞·ª£c ƒë√¢u !\n"
     ]
    }
   ],
   "source": [
    "round_trip = tokenizers.vi.detokenize(tokens)\n",
    "\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thaivu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
