{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Description\n",
    "\n",
    "- In this notebook, I will explore detailed about tokenization in Vietnamese language. Including: Byte Pair Encoding and word-based Tokenization. \n",
    "\n",
    "- The criteria to compare: \n",
    "    - Number of tokens in Vocabulary.\n",
    "    - The ability to handle OOV word.\n",
    "\n",
    "\n",
    "**CONCLUSION**:\n",
    "- Space-based Tokenizer create more tokens in Vocabulary (345_765), compared to BPE (42_000). Furthermore, Space-based Tokenizer create some WEIRD tokens in vocab.\n",
    "\n",
    "=> Byte Pair Encoding is much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 13:38:54.912564: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-10 13:38:54.912584: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-10 13:38:54.912601: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/home/necphy/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:   print(e)\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "from utils.read_file_utils import *\n",
    "from utils.tokenizer_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_EN_FILE_TRAIN = r\"../data/processed_data/en_sent_train.txt\"\n",
    "PATH_VI_FILE_TRAIN = r\"../data/processed_data/vi_sent_train.txt\"\n",
    "\n",
    "PATH_EN_FILE_TEST = r\"../data/processed_data/en_sent_test.txt\"\n",
    "PATH_VI_FILE_TEST = r\"../data/processed_data/vi_sent_test.txt\"\n",
    "\n",
    "PATH_FOLDER_VOCAB = r\"../data/vocab\"\n",
    "\n",
    "FILE_NAME_BPE_TOKENIZER = \"vi_bpe_tokenizer.txt\"\n",
    "FILE_NAME_SPACE_TOKENIZER = \"vi_space_tokenizer.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pair sentence: 2408732\n"
     ]
    }
   ],
   "source": [
    "list_en_sentence_train = read_text_file(PATH_EN_FILE_TRAIN)\n",
    "list_vi_sentence_train = read_text_file(PATH_VI_FILE_TRAIN)\n",
    "\n",
    "assert len(list_en_sentence_train) == len(list_vi_sentence_train)\n",
    "print(f\"Number of pair sentence: {len(list_en_sentence_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = tf.data.Dataset.from_tensor_slices(list_en_sentence_train)\n",
    "train_vi = tf.data.Dataset.from_tensor_slices(list_vi_sentence_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:    is only the beginning .\n",
      "Vietnamese:    ch·ªâ m·ªõi b·∫Øt ƒë·∫ßu th√¥i .\n"
     ]
    }
   ],
   "source": [
    "for en, vi in zip(train_en, train_vi):\n",
    "    print(\"English:   \", en.numpy().decode('utf-8'))\n",
    "    print(\"Vietnamese:   \", vi.numpy().decode('utf-8'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate Vietnamese Vocabulary\n",
    "\n",
    "In this section, we will explore and compare 2 tpye of Tokenization techniques, including BPE and word-based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Byte Pair Encoding \n",
    "\n",
    "a.k.a BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Build and save tokenizer using BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer_params=dict(lower_case=True)\n",
    "bert_tokenizer_params=dict()\n",
    "RESERVED_TOKENS=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "VOCAB_SIE = 100_000  # max number of tokens in vocab\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    vocab_size = VOCAB_SIE,\n",
    "    reserved_tokens=RESERVED_TOKENS,  # Reserved tokens that must be included in the vocabulary\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 24s, sys: 2.21 s, total: 6min 26s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vi_vocab = bert_vocab_from_dataset.bert_vocab_from_dataset(\n",
    "    train_vi.batch(1000).prefetch(tf.data.AUTOTUNE),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '#', '$', '%', '&']\n",
      "['ph·∫£i', 'nƒÉm', 'ƒë·∫øn', 's·ª±', 'c√¥', 'v·ªÅ', 'l·∫°i', 'vi·ªác', 'n√≥i', 't·ª´']\n",
      "['##Ìïú', '##Ìï¥', '##ÌòÑ', '##Ìôî', '##Ô∏è', '##Ôºã', '##Ôºå', '##Ôøº', '##ÔøΩ', '##íÄ≠']\n"
     ]
    }
   ],
   "source": [
    "print(vi_vocab[:10])\n",
    "print(vi_vocab[1000:1010])\n",
    "print(vi_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the Vietnamese vocab: 42473\n",
      "\n",
      "[INFO] Write Vietnamese vocab to file: vi_bpe_tokenizer.txt\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens in the Vietnamese vocab: {len(vi_vocab)}\")\n",
    "print()\n",
    "write_vocab_file(FILE_NAME_BPE_TOKENIZER, vi_vocab)\n",
    "print(f\"[INFO] Write Vietnamese vocab to file: {FILE_NAME_BPE_TOKENIZER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test the BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese token index: <tf.RaggedTensor [[1026, 1433, 1641, 1420, 1406, 2861, 1067]]>\n"
     ]
    }
   ],
   "source": [
    "vi_tokenizer = tf_text.BertTokenizer(FILE_NAME_BPE_TOKENIZER)\n",
    "\n",
    "vi_test = 'th√†nh ph·ªë h·ªì ch√≠ minh ng·∫≠p n∆∞·ªõc'\n",
    "vi_token_idx = vi_tokenizer.tokenize(vi_test)\n",
    "vi_token_idx = vi_token_idx.merge_dims(-2, -1)\n",
    "print(f\"Vietnamese token index: {vi_token_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output ot detokenizer: <tf.RaggedTensor [[b'th\\xc3\\xa0nh', b'ph\\xe1\\xbb\\x91', b'h\\xe1\\xbb\\x93', b'ch\\xc3\\xad',\n",
      "  b'minh', b'ng\\xe1\\xba\\xadp', b'n\\xc6\\xb0\\xe1\\xbb\\x9bc']]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'th√†nh ph·ªë h·ªì ch√≠ minh ng·∫≠p n∆∞·ªõc'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "# txt_tokens = tf.gather(vi_vocab, vi_token_idx)\n",
    "txt_tokens = vi_tokenizer.detokenize(vi_token_idx)\n",
    "print(f\"Output ot detokenizer: {txt_tokens}\")\n",
    "\n",
    "# Join with spaces.\n",
    "original_str = tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1).numpy()[0].decode('utf-8')\n",
    "original_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Text Tokenizer with weird word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ƒëa ƒëoan -> Tokenized: ƒëa ƒëoan\n",
      "Original: ƒë√¨u hiu -> Tokenized: ƒë√¨u hiu\n",
      "Original: lom dom -> Tokenized: lom dom\n",
      "Original: ƒë∆∞·ª£m -> Tokenized: ƒë∆∞·ª£m\n",
      "Original: phi√™u linh -> Tokenized: phi√™u linh\n",
      "Original: ch·∫Øp b√∫t -> Tokenized: ch·∫Øp b√∫t\n",
      "Original: gi·∫•u gi·∫øm -> Tokenized: gi·∫•u gi·∫øm\n",
      "Original: ƒëi·ªÉm xuy·∫øt -> Tokenized: ƒëi·ªÉm xuy·∫øt\n",
      "Original: h√†m s√∫c -> Tokenized: h√†m s√∫c\n",
      "Original: kh·∫≥ng kh√°i -> Tokenized: kh·∫≥ng kh√°i\n",
      "Original: xoay x·ªü -> Tokenized: xoay x·ªü\n",
      "Original: s√∫c t√≠ch -> Tokenized: s√∫c t√≠ch\n"
     ]
    }
   ],
   "source": [
    "list_weird_vietnamese = ['ƒëa ƒëoan', 'ƒë√¨u hiu', 'lom dom', 'ƒë∆∞·ª£m', 'phi√™u linh', 'ch·∫Øp b√∫t', 'gi·∫•u gi·∫øm', 'ƒëi·ªÉm xuy·∫øt', \\\n",
    "                        'h√†m s√∫c', 'kh·∫≥ng kh√°i', 'xoay x·ªü', 's√∫c t√≠ch', \"kh√°nh ki·ªát\", \"tr·∫ßm m·∫∑c\", \"l·ª≠ng l∆°\", \"tr·∫Øc ·∫©n\"]\n",
    "\n",
    "for weird_vi in list_weird_vietnamese:\n",
    "    weird_vi_token_idx = vi_tokenizer.tokenize(weird_vi)\n",
    "    weird_vi_token_idx = weird_vi_token_idx.merge_dims(-2, -1)\n",
    "    weird_vi_txt_tokens = vi_tokenizer.detokenize(weird_vi_token_idx)\n",
    "    weird_vi_original_str = tf.strings.reduce_join(weird_vi_txt_tokens, separator=' ', axis=-1).numpy()[0].decode('utf-8')\n",
    "    print(f\"Original: {weird_vi} -> Tokenized: {weird_vi_original_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>NOTE</font>: BPE handle good OOV word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Space-based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_vocab_space = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_split_text(text):\n",
    "    text = text.decode('utf-8')\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "# def build_space_based_vocab(train_vi):\n",
    "#     vi_vocab_space = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "    \n",
    "#     for idx_sample, vi_sample in enumerate(train_vi):\n",
    "#         vi_sample = vi_sample.numpy().decode('utf-8')\n",
    "#         vi_text = split_text(vi_sample)\n",
    "#         vi_vocab_space.extend(vi_text)\n",
    "\n",
    "#         if idx_sample > 50_000:\n",
    "#             break   \n",
    "#     vi_vocab_space = set(vi_vocab_space)\n",
    "#     return list(vi_vocab_space)\n",
    "\n",
    "\n",
    "def build_space_based_vocab_parallel(train_vi):\n",
    "    \"\"\"\n",
    "    This function is used to build the Vietnamese vocab space-based, using the train_vi dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Create input argument\n",
    "    input_arg = []\n",
    "    for idx_sample, vi_sample in enumerate(train_vi):\n",
    "        vi_sample = vi_sample.numpy()\n",
    "        input_arg.append(vi_sample)\n",
    "        \n",
    "    # 2. Decode and split text in parallel\n",
    "    pool = multiprocessing.Pool(8)\n",
    "    total_vi_vocab_space = pool.map(decode_and_split_text, input_arg)\n",
    "    pool.close()\n",
    "    \n",
    "    # 3. Build vocab space\n",
    "    vi_vocab_space = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "    for vi_text in total_vi_vocab_space:\n",
    "        vi_vocab_space.extend(vi_text)      \n",
    "    vi_vocab_space = set(vi_vocab_space)  # Remove duplicate tokens\n",
    "    \n",
    "    return list(vi_vocab_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the Vietnamese vocab: 345765\n",
      "\n",
      "['v·ª±cc√°c', 'maclean', 'marielagriffor', 'armour)', 'c√°cƒë·∫≠p', '2013-2017:', '(Âùä‰∏ª„ÇÅ„Åè„Çä)', 'sewell', 'antechamber', 'bul√¥ng-4']\n",
      "['l√©c', 'nolfox', '2017m·ªôt', 'evgenii', 'appel', 'phuthi', 'sunbul', '(ÌôçÏùµÎåÄÌïôÍµê)', 'preparen', '\"slide\"']\n",
      "['tinl√†', 'bokura', 'kaliningrad\"', '(enac)', 'beta-secretase', 'brookahven', '‡®∞‡®æ‡®ñ‡®æ\"', 'yeigo', 'littoral', 'ancestral']\n",
      "CPU times: user 49.3 s, sys: 3.36 s, total: 52.7 s\n",
      "Wall time: 48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vi_vocab_space = build_space_based_vocab(train_vi)\n",
    "vi_vocab_space = build_space_based_vocab_parallel(train_vi)\n",
    "\n",
    "print(f\"Number of tokens in the Vietnamese vocab: {len(vi_vocab_space)}\")\n",
    "print()\n",
    "print(vi_vocab_space[:10])\n",
    "print(vi_vocab_space[1000:1010])\n",
    "print(vi_vocab_space[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Write Vietnamese vocab to file: vi_space_tokenizer.txt\n"
     ]
    }
   ],
   "source": [
    "write_vocab_file(FILE_NAME_SPACE_TOKENIZER, vi_vocab_space)\n",
    "print(f\"[INFO] Write Vietnamese vocab to file: {FILE_NAME_SPACE_TOKENIZER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Tokenizer by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese token index: <tf.RaggedTensor [[307926, 194886, 216521, 320506, 5014]]>\n"
     ]
    }
   ],
   "source": [
    "vi_tokenizer_space = tf_text.BertTokenizer(FILE_NAME_SPACE_TOKENIZER)\n",
    "\n",
    "# vi_test = 'hoa ph∆∞·ª£ng ƒë·ªè l√† tu·ªïi t√¥i m∆∞·ªùi t√°m, th·∫ßm l·∫∑ng ai hay m·ªëi t√¨nh ƒë·∫ßu'\n",
    "vi_test = 'haluliii l√† m·ªëi t√¨nh ƒë·∫ßu'\n",
    "vi_token_idx = vi_tokenizer_space.tokenize(vi_test)\n",
    "vi_token_idx = vi_token_idx.merge_dims(-2, -1)\n",
    "print(f\"Vietnamese token index: {vi_token_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output ot detokenizer: <tf.RaggedTensor [[b'[UNK]', b'l\\xc3\\xa0', b'm\\xe1\\xbb\\x91i', b't\\xc3\\xacnh',\n",
      "  b'\\xc4\\x91\\xe1\\xba\\xa7u']]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[UNK] l√† m·ªëi t√¨nh ƒë·∫ßu'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "txt_tokens = vi_tokenizer_space.detokenize(vi_token_idx)\n",
    "print(f\"Output ot detokenizer: {txt_tokens}\")\n",
    "\n",
    "# Join with spaces.\n",
    "original_str = tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1).numpy()[0].decode('utf-8')\n",
    "original_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ƒëa ƒëoan -> Tokenized: ƒëa ƒëoan\n",
      "Original: ƒë√¨u hiu -> Tokenized: ƒë√¨u hiu\n",
      "Original: lom dom -> Tokenized: lom dom\n",
      "Original: ƒë∆∞·ª£m -> Tokenized: ƒë∆∞·ª£m\n",
      "Original: phi√™u linh -> Tokenized: phi√™u linh\n",
      "Original: ch·∫Øp b√∫t -> Tokenized: ch·∫Øp b√∫t\n",
      "Original: gi·∫•u gi·∫øm -> Tokenized: gi·∫•u gi·∫øm\n",
      "Original: ƒëi·ªÉm xuy·∫øt -> Tokenized: ƒëi·ªÉm xuy·∫øt\n",
      "Original: h√†m s√∫c -> Tokenized: h√†m s√∫c\n",
      "Original: kh·∫≥ng kh√°i -> Tokenized: kh·∫≥ng kh√°i\n",
      "Original: xoay x·ªü -> Tokenized: xoay x·ªü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: s√∫c t√≠ch -> Tokenized: s√∫c t√≠ch\n",
      "Original: kh√°nh ki·ªát -> Tokenized: kh√°nh ki·ªát\n",
      "Original: tr·∫ßm m·∫∑c -> Tokenized: tr·∫ßm m·∫∑c\n",
      "Original: l·ª≠ng l∆° -> Tokenized: l·ª≠ng l∆°\n",
      "Original: tr·∫Øc ·∫©n -> Tokenized: tr·∫Øc ·∫©n\n"
     ]
    }
   ],
   "source": [
    "list_weird_vietnamese = ['ƒëa ƒëoan', 'ƒë√¨u hiu', 'lom dom', 'ƒë∆∞·ª£m', 'phi√™u linh', 'ch·∫Øp b√∫t', 'gi·∫•u gi·∫øm', 'ƒëi·ªÉm xuy·∫øt', \\\n",
    "                        'h√†m s√∫c', 'kh·∫≥ng kh√°i', 'xoay x·ªü', 's√∫c t√≠ch', \"kh√°nh ki·ªát\", \"tr·∫ßm m·∫∑c\", \"l·ª≠ng l∆°\", \"tr·∫Øc ·∫©n\"]\n",
    "\n",
    "for weird_vi in list_weird_vietnamese:\n",
    "    weird_vi_token_idx = vi_tokenizer_space.tokenize(weird_vi)\n",
    "    weird_vi_token_idx = weird_vi_token_idx.merge_dims(-2, -1)\n",
    "    weird_vi_txt_tokens = vi_tokenizer_space.detokenize(weird_vi_token_idx)\n",
    "    weird_vi_original_str = tf.strings.reduce_join(weird_vi_txt_tokens, separator=' ', axis=-1).numpy()[0].decode('utf-8')\n",
    "    print(f\"Original: {weird_vi} -> Tokenized: {weird_vi_original_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>NOTE</font>: \n",
    "- Space-based Tokenizer create more tokens in Vocabulary, compared to BPE.\n",
    "- Furthermore, It create some WEIRD tokens in vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
