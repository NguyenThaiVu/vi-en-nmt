{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, we will build the **Machine Translation** from English to Vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:   print(e)\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "from utils.read_file_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.visualize_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_EN_FILE_TRAIN = r\"data/processed_data/en_sent_train.txt\"\n",
    "PATH_VI_FILE_TRAIN = r\"data/processed_data/vi_sent_train.txt\"\n",
    "\n",
    "PATH_EN_FILE_TEST = r\"data/processed_data/en_sent_test.txt\"\n",
    "PATH_VI_FILE_TEST = r\"data/processed_data/vi_sent_test.txt\"\n",
    "\n",
    "PATH_TOKENIZER = r\"data/tokeninzer_en_vi_converter\"\n",
    "\n",
    "MAX_TOKENS = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data handling\n",
    "\n",
    "- First, we need to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_en_sentence_train = read_text_file(PATH_EN_FILE_TRAIN)\n",
    "list_vi_sentence_train = read_text_file(PATH_VI_FILE_TRAIN)\n",
    "\n",
    "list_en_sentence_val = read_text_file(PATH_EN_FILE_TEST)\n",
    "list_vi_sentence_val = read_text_file(PATH_VI_FILE_TEST)\n",
    "\n",
    "assert len(list_en_sentence_train) == len(list_vi_sentence_train)\n",
    "assert len(list_en_sentence_val) == len(list_vi_sentence_val)\n",
    "\n",
    "print(f\"Number of training sample: {len(list_en_sentence_train)}\")\n",
    "print(f\"Number of validation sample: {len(list_en_sentence_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = tf.data.Dataset.from_tensor_slices((list_en_sentence_train, list_vi_sentence_train))\n",
    "val_examples = tf.data.Dataset.from_tensor_slices((list_en_sentence_val, list_vi_sentence_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en_examples, vi_examples in train_examples.batch(5).take(1):\n",
    "    print('> Examples in English:')\n",
    "    for en in en_examples.numpy():\n",
    "        print(en.decode('utf-8'))\n",
    "    print()\n",
    "\n",
    "    print('> Examples in Vietnamese:')\n",
    "    for vi in vi_examples.numpy():\n",
    "        print(vi.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load tokenizer\n",
    "\n",
    "- We load the pre-train Tokenizer and test it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.saved_model.load(PATH_TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizers.vi.tokenize(vi_examples)\n",
    "\n",
    "print('> This is a padded-batch of token IDs:')\n",
    "for row in encoded.to_list():\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `detokenize` method can convert these tokens index back to original text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_original_sentence = tokenizers.vi.detokenize(encoded)\n",
    "\n",
    "for original_sentence in list_original_sentence.numpy():\n",
    "    print(original_sentence.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Set up data pipeline with `tf.data`\n",
    "\n",
    "The following function takes batches of text as input, and converts them to a format suitable for training.\n",
    "\n",
    "- It tokenizes them into ragged batches.\n",
    "- It trims each to be no longer than MAX_TOKENS.\n",
    "- It splits the target (Vietnamese) tokens into inputs and labels. These are shifted by one step so that at each input location the label is the id of the next token.\n",
    "- It converts the RaggedTensors to padded dense Tensors.\n",
    "- It returns an (inputs, labels) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(en, vi):\n",
    "    \"\"\"\n",
    "    This function take pair of en and vi. Then return suitable format for training\n",
    "    \"\"\"\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)      # Output is ragged.\n",
    "    en = en[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    en = en.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    vi = tokenizers.vi.tokenize(vi)\n",
    "    vi = vi[:, :(MAX_TOKENS+1)]\n",
    "    vi_inputs = vi[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    vi_labels = vi[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return (en, vi_inputs), vi_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def make_batches(ds):\n",
    "  return (ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "# Create training and validation set batches.\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (pt, en), en_labels in train_batches.take(1):\n",
    "  break\n",
    "\n",
    "print(pt.shape)\n",
    "print(en.shape)\n",
    "print(en_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.vi.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can test the output shape of Transformer model.\n",
    "- Then, we can test the output shape of the attention score, which has shape `(batch, heads, target_seq, input_seq)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer((pt, en))\n",
    "\n",
    "print(en.shape)\n",
    "print(pt.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
    "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Custom optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Loss and metrics\n",
    "\n",
    "- Since the target sequences are padded, it is important to apply a padding mask when calculating the loss. \n",
    "- In other word, we will remove the padding 0 when calculating the mask. That mean the padding 0 will not affect the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "transformer.fit(train_batches,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference\n",
    "\n",
    "The following steps are used for inference:\n",
    "- Encode the input sentence using the Vietnamese tokenizer (tokenizers.pt). This is the encoder input.\n",
    "- The decoder input is initialized to the [START] token.\n",
    "- Calculate the padding masks and the look ahead masks.\n",
    "- The decoder then outputs the predictions by looking at the encoder output and its own output (self-attention).\n",
    "- Concatenate the predicted token to the decoder input and pass it to the decoder **AGAIN**.\n",
    "- The process will stop until we get the [END] token or reach the maximum number of token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "\n",
    "    # The input sentence is English, hence adding the `[START]` and `[END]` tokens.\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.tokenizers.en.tokenize(sentence).to_tensor()\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # As the output language is Vietnamese, initialize the output with the `[START]` token.\n",
    "    start_end = self.tokenizers.vi.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis]\n",
    "    end = start_end[1][tf.newaxis]\n",
    "\n",
    "    # Define the output_array with [START] token\n",
    "    # `tf.TensorArray` is required here, so that the dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "    predicted_prob_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "      predicted_prob_array = predicted_prob_array.write(i, predictions[0, 0, :])  # Write predictions to TensorArray\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack()) # Shape (1, tokens)\n",
    "    list_predicted_prob = predicted_prob_array.stack()  # Convert to tensor after loop\n",
    "\n",
    "    # We get the predicted output and corresponding token\n",
    "    text = tokenizers.vi.detokenize(output)[0]  # Shape: `()`.\n",
    "    tokens = tokenizers.vi.lookup(output)[0]\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text, tokens, attention_weights, list_predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I am a student.'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights, list_predicted_prob = translator(tf.constant(sentence))\n",
    "print(translated_text.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Attention plots\n",
    "\n",
    "- To increase the interpretable of the model, we create attention heatmaps you can use to visualize the internal working of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The capital of Vietnam is Hanoi, which is a beautiful city.'\n",
    "sentence = sentence.lower()\n",
    "\n",
    "translated_text, translated_tokens, attention_weights, list_predicted_prob = translator(tf.constant(sentence))\n",
    "print(translated_text.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tokens = tf.convert_to_tensor([sentence])\n",
    "in_tokens = tokenizers.en.tokenize(in_tokens).to_tensor()\n",
    "in_tokens = tokenizers.en.lookup(in_tokens)[0]\n",
    "in_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will get the `attention_weights` of first head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_weights(in_tokens, translated_tokens, attention_weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can calculate average of all attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_attention_weights = calculate_average_attention(attention_weights[0])\n",
    "\n",
    "plot_attention_head(in_tokens, translated_tokens, average_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Translation Confidence Estimation\n",
    "\n",
    "In this section, we will explore the uncertainty of the predicted tokens using entropy. Intuition:\n",
    "- Low entropy (low uncertainty) -> High confidence\n",
    "- High entropy -> Low confidence\n",
    "\n",
    "Entropy Threshold:\n",
    "- We will use the entropy threshold = `log(k)/2`, where `k` is the number of possible categories. Since the maximum value of entropy is `log(k)`.\n",
    "- In other case, we can loop through all samples in traning set and calculate the average entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_confidence_estimation(translated_tokens[1:], list_predicted_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Export the model\n",
    "\n",
    "Next, we can save the model into `tf.saved_model` to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def __call__(self, sentence):\n",
    "    (result,\n",
    "     tokens,\n",
    "     attention_weights, list_predicted_prob) = self.translator(sentence, max_length=MAX_TOKENS)\n",
    "    \n",
    "    return result, tokens, attention_weights, list_predicted_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_export = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(translator_export, export_dir='translator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load back the saved translator to make sure it work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load('translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text, translated_tokens, attention_weights, list_predicted_prob = reloaded('i love cake, because it is sweet.')\n",
    "print(translated_text.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
